\documentclass[10pt,a4paper]{article}
\usepackage{comment}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsthm,amsfonts,mathrsfs}
\usepackage{enumerate}

\usepackage{amsmath,accents}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage{latexsym}
\usepackage{subfigure}
\usepackage{hyperref,cleveref}
\usepackage{nomencl}
\usepackage[dvipsnames]{xcolor}
\usepackage[normalem]{ulem}
\usepackage{bold-extra}
\usepackage{tabto}
\usepackage{tikz}
\usepackage{titlesec}
\newcommand{\eqname}[1]{\tag*{#1}}
\setcounter{secnumdepth}{4}
\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\usetikzlibrary{shapes,arrows}
\tikzstyle{decision} = [diamond, draw,
    text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20,
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
    minimum height=2em]
\setlength{\parindent}{0pt}
\TabPositions{5.cm}
\newcommand{\nomunit}[1]{\renewcommand{\nomentryend}{\hspace*{\fill}#1}}

\newcommand{\partder}[2]{\ensuremath{\frac{\partial #1}{\partial #2}}}
\newcommand{\secpartder}[2]{\ensuremath{\frac{\partial^2 #1}{\partial #2^2}}}
\newcommand{\nthpartder}[3]{\ensuremath{\frac{\partial^{#1}
#2}{\partial #3^{#1}}}}
\newcommand{\fullder}[2]{\ensuremath{\frac{\mbox{d} #1}{\mbox{d} #2}}}
\newcommand{\secfullder}[2]{\ensuremath{\frac{\mbox{d}^2 #1}{\mbox{d} #2^2}}}
\newcommand{\nfullder}[3]{\ensuremath{\frac{\mbox{d}^{#1} #2}{\mbox{d}
#3^{#1}}}}
\newcommand{\mixedder}[3]{\ensuremath{\frac{\partial^{2} #1}{\partial
#2 \partial #3}}}
\newcommand{\df}[1]{\, \ensuremath{\mbox{d}#1}}
\newcommand{\grad}{\, \mbox{grad} \,}
\newcommand{\dive}{\, \mbox{div} \,}
\newcommand{\real}[1]{\text{Re} \left\{ #1 \right\}}
\newcommand{\imag}[1]{\text{Im}\left\{ #1 \right\}}
\newcommand{\commentstm}[1]{\textcolor{blue}{\textbf{STM:\ #1}}}
\newcommand{\commentstmtwo}[1]{\textcolor{purple}{\textbf{STM:\ #1}}}
\newcommand{\newstm}[1]{\textcolor{red}{\textbf{#1}}}
\newcommand{\newstmtwo}[1]{\textcolor{orange}{\textbf{#1}}}
\newcommand{\oldstm}[1]{\sout{#1}}
\newcommand{\oldstmtwo}[1]{\xout{#1}}
\newcommand{\eqlabelsigi}[1]{\label{eq:#1}  \tag*{\texttt{#1}}}
\newcommand{\brok}{\textcolor{ForestGreen}{\textit{\textbf{OK for Bernhard}}}}
\newcommand{\apok}{\textcolor{ForestGreen}{\textit{\textbf{OK for Andr\'{e}}}}}
\newcommand{\brnok}{\textcolor{red}{\textit{\textbf{NOT OK for Bernhard}}}}
\newcommand{\apnok}{\textcolor{red}{\textit{\textbf{NOT OK for Andr\'{e}}}}}

\newcommand{\commentbr}[1]{\textcolor{orange}{\textbf{BR:\ #1}}}


\newcommand{\xs}{\mathbf{x}_\text{s}}
\newcommand{\xr}{\mathbf{x}_\text{r}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\n}{\mathbf{n}}
\newcommand{\s}{\mathbf{s}}

\newtheorem{thm}{Theorem}[section]
\makenomenclature
%opening

\title{Current implementation of Conjugate Gradient (with Regularisation)}
\author{ALTEN Netherlands\\
Morelli, L.}

\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage

\clearpage
\section{Preface}
Here we will describe the current implementation of ConjugateGradientInversion and its refactored version ConjugateGradientWithRegularisationCalculator, highlighting the differences between the documentation ( \texttt{\detokenize{.../doc/ReadMe/1_ProjectDescription.pdf}} and \newline \texttt{\detokenize{.../doc/BackGroundInfo/phd-Peter-Haffinger-seismic-broadband-full-waveform-inversion.pdf}} ) and the code.

Throughout this document, we will refer as the PhD thesis from Peter Haffinger simply as 'Thesis'.

The main body of the text is taken from \texttt{\detokenize{.../doc/ReadMe/1_ProjectDescription.pdf}}, and only subsection \ref{CG_solver} will be modified.

There might be a discrepancy in a few variables' names, as during the refactor some have been made member variables of the class, thus obtaining a $'\_'$ character at the beginning of their names.

Moreover, whenever Line numbers are mentioned, we refer to the original \textbf{conjugateGradientInversion.cpp} file.

\section{Introduction}
Please consult  \texttt{\detokenize{.../doc/ReadMe/1_ProjectDescription.pdf}} for the full document. 
The parts that have been added are highlighted in \textcolor{red}{red} and appear in subsection 3.3.2.

\section{The Model}
\subsection{Physical model}

The mathematical model outlines the equations used in the code for the FWI.
We begin with the Green's function and apply it to the
simplest acoustic equation - the Helmholtz equation.
For the acoustic case with constant density, the wave equation can be
described by two equations,
being the data equation and the object equation. The data equation
describes the seismic dataset, in terms of a total field
$p_{\text{tot}}$ at each grid point in the subsurface, the contrast
function $\chi$ and the Green's function $\mathcal{G}$
in a background medium: 

\begin{align}
\label{eq:calculateData}
p_\text{data}(\mathbf{x_\text{r}},\mathbf{x_\text{s}},\omega) = \int
\mathcal{G}(\xr, \x, \omega) p_\text{tot}(\x, \xs, \omega) \chi(\x)
\df{\x}\\
\tag*{\texttt{eq:calculateData, see include/inversion.h\footnotemark}}
\end{align}\footnotetext{``eq:'' denotes equations that are implemented in the code, ``step:'' denotes equations we present to understand the flow of this manual. Inside the code, the implementation of these equations is marked with such symbols. There, ``rel:'' means that an equation is related to the ``eq'', similar to the use of ``step'' here.}

Reading from right to left, equation \ref{eq:calculateData} can be understood as
follows: A source transmits a wavefield that propagates to every point
in the subsurface. Note that this wavefield $p_\text{tot}$ is
generally quite complex because it takes the interaction of all
scatterers in the subsurface already into account. This wavefield is
creating secondary sources in all points where the contrast function
$\chi$ is non-zero. The secondary sources transmit energy through a
smooth background medium to the receivers, represented by the Green's
function $\mathcal{G}$ in equation \ref{eq:calculateData}.\\ %\linebreak
%\newline
The measured seismic data at every receiver are then a summation of
all secondary sources. It should be mentioned that direct waves,
including ground roll and surface waves are supposed to be removed
from the measured data to obtain $p_\text{data}$.

We use the 2-D case for the Helmholtz equation. Further, the Green's
function is calculated for this equation. The contrast function has to
be determined. Further the conjugate scheme is established to
determine the error functional.

Measured seismic data always contains some form of noise, so the
inversion process is required to be regularised. Therefore, we extend
the conjugate gradient scheme so as to include a multiplicative regularisation factor.

\subsubsection{The Green's function}
A Green's function is the impulse response of an inhomogeneous linear
differential equation defined on a domain, with specified initial
conditions or boundary conditions.
The Green's function of this equation is defined as the solution
$\mathcal{G}(\mathbf{x}, \mathbf{y})$, of the equation

\begin{align}
\label{eq:GeneralGreensFunc}
[ \nabla^2 \mathcal{G}(\mathbf{x}, \mathbf{y}) + k_0^2
\mathcal{G}(\mathbf{x}, \mathbf{y}) = -\delta(\mathbf{x} -
\mathbf{y}). ]\\
\tag*{\texttt{step:GeneralGreensFunc}}
\end{align}

The solution to (\ref{eq:GeneralGreensFunc}) is then given by

\begin{align} \label{eq:GeneralGreensSol}
[ u_\text{ind}(\mathbf{x}) =
\int_{\mathbf{y} \in \mathbb{R}^n} \mathcal{G}(\mathbf{x}, \mathbf{y})
f_\text{ind}(\mathbf{y}) \df{\mathbf{y}}.\\
\tag*{\texttt{step:GeneralGreensSol}}
\end{align}

The interesting cases are the 2D and 3D case. We will focus on the 2D case. The Green's function is then
given by

\begin{align} \label{eq:GreensFunc2d}[ \mathcal{G}(\mathbf{x}, \mathbf{y}) =
\frac{\imath}{4} H_0^{(1)}(k_0 \|\mathbf{x} - \mathbf{y}\|) =
-\frac{1}{4} Y_0(k_0 \|\mathbf{x} - \mathbf{y}\|) + \frac{\imath}{4}
J_0(k_0 \|\mathbf{x} - \mathbf{y}\|).  ]\\
\tag*{\texttt{eq:GreensFunc2d, see greensFunctions.h}}
\end{align}

In the above equation, the $H_0$ , $J_0$ and $Y_0$ are defined as the
Henkel's functions and can be further researched at :
\url{http://amcm.pcz.pl/get.php?article=2012_1/art_06.pdf}

\subsubsection{Helmholtz equation}
The simplest model we can use is the acoustic Helmholtz equation. We
use a scalar pressure field $u(\mathbf{x})$ and the domain is modeled
using $\chi(\mathbf{x})$. and is called the contrast and can be
directly related to the wave speed at that point in the domain.
Mathematically the equation has the form:

\begin{align} \label{eq:generalHelmholtzFunc}
\nabla^2 u(\mathbf{x}) + k(\mathbf{x})^2 u(\mathbf{x}) =
-f_{\text{ext}}(\mathbf{x}).\\
\tag*{\texttt{step:generalHelmholtzFunc}}
\end{align}

The wave number $k$ is given by $k = \frac{\omega}{c}$ where $\omega$
is the angular frequency and c the
acoustic wave velocity in m/s of the true medium.
We split the pressure field into $u(\mathbf{x}) = u_0(\mathbf{x}) +
u_{\text{ind}}(\mathbf{x})$, where $u_0(\mathbf{x})$ is defined as the
field given by the background velocity and external sources,

\begin{align} \label{eq:splitHelmholtzU0}
\nabla^2 u_0(\mathbf{x}) + k_0^2 u_0(\mathbf{x}) = -f_{\text{ext}}(\mathbf{x}).\\
\tag*{\texttt{step:splitHelmholtzU0}}
\end{align}

Substituting in (\ref{eq:splitHelmholtzU0}) results in

\begin{align} \label{eq:splitHelmholtzUind}
\nabla^2 u_\text{ind}(\mathbf{x}) + k_0^2 u_\text{ind}(\mathbf{x}) =
-f_\text{ind}(\mathbf{x}),\\
\tag*{\texttt{step:splitHelmholtzUind}}
\end{align}

with $f_\text{ind}(\mathbf{x}) = k_0^2 \chi(\mathbf{x}) u(\mathbf{x})$
the induced source term.

\subsubsection{The Contrast function}
The contrast function is defined as:

\begin{align} \label{eq:contrastFunc}\chi(\mathbf{x}) = 1 -
\left(\frac{c0(\vec{x})}{c(\vec{x})} \right)^2.\\
\tag*{\texttt{step:contrastFunc}}
\end{align}

It depends on the difference between a known background medium
$c_\text{0}(\vec{x})$ and the true, but unknown, subsurface model
$c(\vec{x})$. The total field on the right-hand side in equation
\ref{eq:calculateData} is dependent on the contrast, because it contains the
interaction between all subsurface scatterers. Then it follows that
there is a nonlinear relationship between the subsurface properties
and the measured seismic data.

Notice that the contrast is generally unknown, and will be
approximated using an iterative scheme. During each step the induced
source is considered constant and known so we basically solve the same
equation as (\ref{eq:GeneralGreensSol}) each step.

\subsection{Finite Difference Approach}
Instead of solving the Helhmoltz equation using Green's functions, it is also possible to solve the equation using a finite difference approach. In 2D the domain $\Omega=[x_{\min},x_{\max}]\times [z_{\min},z_{\max}]$ is discretized as follows:

\begin{equation}
G_{h_x,h_z} = \{(x_i,z_j): x_i = x_{\min}+ ih_x, z_j=z_{\min}+jh_z; 1\leq i,j\leq n\},
\end{equation} 

where $h_x$ and $h_z$ denote the cell size in their respective directions. Note that the positive direction for $x$ is from left to right ($x\rightarrow$), and for $z$ downwards ($z\downarrow$). For internal grid points, i.e. points not on the boundary, the discretized Helmholtz equation is given as 

\begin{equation} \label{eqn:discret}
\frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{h_x^2} + \frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{h_z^2} + k_{ij}^2u_{ij}= f_{ij},
\end{equation}

where $u_{i,j}=u(x_i,z_j)$. Boundary conditions are required to minimize reflections. There are three basic approaches.
\begin{enumerate}
	\item Add a damping term $i\omega\alpha u$ to the Helmholtz equation, add an extra layer to the domain and set $\alpha$ increasing towards the boundary.
	\item Use Absorbing Boundary Conditions (ABC), which are boundary conditions that filter out waves.
	\item Use Perfectly Matching Layers (PML), which is basically a more sophisticated version of the first option.
\end{enumerate}

The problem with the first option is that $\alpha$ needs tweaking based on $\omega$. ABC can only prevent some reflections since they are bad at preventing reflections from waves traveling almost tangential to the boundary. Therefore, PML seems to be the best option \cite{Comparison}. Both the first order ABC and PML are implemented. Second order ABC are not implemented.\footnote{For the theory, see \cref{sec:ABC}.} The user can choose which approach to use by adjusting input card parameters (see \cref{sec:FDimplementation} for more information). In practice, ABC perform better than PML. Implementing second order ABC may perform even better.

\subsubsection{First Order ABC}
ABC can be used to (partially) prevent reflections. First we assume that the index of refraction $n(\x)\equiv C_1$ close to the boundary for some $C_1\in \mathbb{R}$. The main idea of ABC is that close to the boundary the solution to the full problem can be approximated by a plane wave that propagates in a direction normal to the boundary \cite{LectNotes}, i.e.

\begin{equation}
u(\x) \approx Ce^{i\omega C_1 \hat{\n}\cdot \x},
\end{equation}

where $\hat{\n}$ denotes the outward pointing normal of the boundary. Now consider, for example, the upper boundary $\Gamma_0$. We want the ABC to block all outgoing waves, so that they cannot cause reflections. Since we assumed that waves close to the boundary can be approximated by a wave traveling in a direction normal to the boundary, we want the waves with direction $[0,-1]^\top$ to be blocked. Now

\begin{equation}
\frac{\partial u}{\partial \hat{\n}} -i\omega C_1 u = i\omega CC_1e^{-i\omega C_1z} - i\omega CC_1e^{-i \omega C_1 z} = 0
\end{equation}

for $\hat{\n}=[0,-1]^\top$ and for all $C\in\mathbb{R}$\footnote{Since, for $\hat{\n} = [\hat{n}_1,\hat{n}_2]^\top$, we have 
	\begin{equation*}
	\frac{\partial u}{\partial \hat{\n}} -i\omega nu= \nabla u\cdot \hat{\n} -i\omega nu= i\omega nC (\hat{n}_1^2 e^{i\omega n(\hat{n}_1x+\hat{n}_2z)} + \hat{n}_2^2e^{i\omega n(\hat{n}_1x+\hat{n}_2z)})-i\omega nCe^{i\omega n(\hat{n}_1x+\hat{n}_2z)}
	\end{equation*}}. Hence the ABC can be formulated as

\begin{equation}
\frac{\partial u}{\partial \hat{\n}} - i\omega nu=0,
\end{equation}

The ABC formulated above are first order accurate in the angle at which the wave strikes the boundary \cite{AbsorpationRates}.

\subsubsection{Discretization}
For points on the boundary not all neighbours exist. Consider, for example, the top left boundary point. There $u_{i-1,j}$ and $u_{i,j+1}$ do not exist. From the boundary conditions it follows that

\begin{eqnarray}
&& \frac{\partial u}{\partial \hat{\n}} - i\omega nu = -\frac{\partial u}{\partial x} - i\omega nu =0\\
&\leadsto& -\frac{u_{i+1,j}-u_{i-1,j}}{2h_x}-i\omega n_{ij} u_{ij}=0\\
&\implies& u_{i-1,j} = 2h_xi\omega n_{ij}u_{ij}+u_{i+1,j}
\end{eqnarray}

where $\hat{\n}=[-1,0]^\top$ is the outward pointing normal and 

\begin{eqnarray}
&& \frac{\partial u}{\partial \hat{\n}} - i\omega nu = -\frac{\partial u}{\partial z} - i\omega nu =0\\
&\leadsto& -\frac{u_{i,j+1}-u_{i,j-1}}{2h_z}-i\omega n_{ij} u_{ij}=0\\
&\implies& u_{i,j-1} = 2h_zi\omega n_{ij}u_{ij}+u_{i,j+1}
\end{eqnarray}

where $\hat{\n}=[0,-1]^\top$ is the outward pointing normal (since the positive $z$ direction is downwards). Here we used a central difference discretization. By substituting these relations into \cref{eqn:discret} the final discretization for the boundary point is obtained. 

\subsubsection{Perfectly Matched Layers}
The basic idea of Perfectly Matched Layers is that an artificial boundary layer is introduced in which the waves decay exponentially. This is done via a coordinate transformation of the Helmholtz equation such that in the original domain the solutions correspond to the solutions of the original equation and in the newly introduced artificial boundary layer the to exponentially decaying ones \cite{Erlangga2008}. The following coordinate transformation is used for both $x$ and $z$.

\begin{equation}
\frac{\partial }{\partial y} \mapsto \frac{1}{S(y)}\frac{\partial}{\partial y},
\end{equation}

where 

\begin{equation}
S(y)=1+\frac{\sigma(y)}{i\omega n}
\end{equation}

By choosing $\sigma_y(y)$ equal to the square of the distance into the PML, the desired properties are obtained. As a consequence of the coordinate transformation we have to solve the adjusted Helmholtz equation

\begin{equation} \label{eqn:PMLHelmholtz}
\frac{\partial }{\partial x}\frac{S(z)}{S(x)}\frac{\partial }{\partial x}u + \frac{\partial }{\partial z}\frac{S(x)}{S(z)}\frac{\partial }{\partial z}u + n^2\omega^2 S(x)S(z)u=f(x,z),
\end{equation}

where we assume $u\equiv 0$ (Dirichlet) on the outer boundary. In the original domain the equation above is simply reduced to the original Helmholtz discretization.

\subsubsection{Discretization}
Within the original domain, the discretization in \cref{eqn:discret} can be used. In order to obtain the discretization of \cref{eqn:PMLHelmholtz} within the PML, we first have to expand it. 

\begin{eqnarray}
&& \frac{\partial }{\partial x}\frac{S(z)}{S(x)}\frac{\partial }{\partial x}u + \frac{\partial }{\partial z}\frac{S(x)}{S(z)}\frac{\partial }{\partial z}u + n^2\omega^2 S(x)S(z)u\\
&=& \frac{S(z)}{S(x)}\frac{\partial^2 u}{\partial x^2}-\frac{S(z)}{S^2(x)}S'(x)\frac{\partial u}{\partial x} + \frac{S(x)}{S(z)}\frac{\partial^2 u}{\partial z^2}-\frac{S(x)}{S^2(z)}S'(z)\frac{\partial u}{\partial z}+ n^2\omega^2 S(x)S(z)u
\end{eqnarray}

By using central difference discretizations for the first and second derivatives of $u$ we obtain the following discretization.

\begin{eqnarray}
&&\frac{S(z_j)}{S(x_i)}\frac{u_{i+1,j}-2u_{ij}+u_{i-1,j}}{h_x^2}-\frac{S(z_j)}{S^2(x_i)}S'(x_i)\frac{u_{i+1,j}-u_{i-1,j}}{2h_x}\\
&+& \frac{S(x_i)}{S(z_j)}\frac{u_{i,j+1}-2u_{ij}+u_{i,j-1}}{h_z^2}-\frac{S(x_i)}{S^2(z_j)}S'(z_j)\frac{u_{i,j+1}-u_{i,j-1}}{2h_z}\\
&+& n_{ij}^2\omega^2 S(x_i)S(z_j)u_{ij}=f_{ij}
\end{eqnarray}

\subsubsection{Source Implementation}
The easiest method to implement the source is by just using a point source, that is, a source located at one precise grid point. In order to achieve this the source coordinates are rounded to the nearest grid point. For a point source, the integral over the cell size should equal $1$, hence the point source value equals $1/h_xh_z$. This is necessary to calculate the pressure field using the finite difference model. The problem with this source implementation is that errors are introduced do due rounding the source positions to the nearest grid point. Especially when combing different forward and inversion models, one would want to avoid this.\\

For this reason another source implementation is added which should more accurately reflect the actual position of the source. This implementation is described in \cite{Hicks}. Basically, the source function used is given by

\begin{equation}
	\frac{\sin(\pi d(x))}{\pi d(x)}\frac{\sin(\pi d(z))}{\pi d(z)}
\end{equation}

where $d(y)$ denotes the distance in grid points from the source position. This source function is then multiplied by a so-called window function which basically determines over how many grid points the source is spread out. The final implementation then becomes

\begin{equation}
	\frac{I_0(\beta \sqrt{1-(d(x)/r)^2})}{I_0(\beta)}\frac{\sin(\pi d(x))}{\pi d(x)}\frac{I_0(\beta \sqrt{1-(d(z)/r)^2})}{I_0(\beta)}\frac{\sin(\pi d(z))}{\pi d(z)},
\end{equation}

where $I_0(y)$ denotes the Bessel function of the first kind. The parameters $\beta$ and $r$ denote the shape and half-width, respectively. The half-width gives the range in grid points from the actual source position in which the source term will be non-zero. The parameters $r=4$ and $\beta=6.31$ are a good choice under most circumstances \cite{Hicks}.\\

For both source implementations the original domain is enlarged so that it includes all the sources. Note that for $r>0$ addition grid points are required to ensure the whole source is included in the domain.

\subsubsection{Input Card Parameters} \label{sec:FDimplementation}
Finite difference forward model has its own input card containing four parameters. First the PMLWidthFactor, which determines the width of the PML by multiplying this factor in the $x$ and $z$ direction by the wavelength of the background subsurface. Setting this parameter equal to $0.0$ in both directions entails that ABC are used instead of PML, which is the default setting because in practice ABC performs better than PML.\\

The SourceParameter determines the half-width $r$ (which is a natural number) and shape $\beta$ for the sine source implementation as given in the section above and \cite{Hicks}. By default the parameters are set to $r=4$ and $\beta=6.31$ for the reason given in previous section. When setting $r=0$, the point source implementation will be used.

\subsubsection{Python Scripts}
Python scripts were written for prototyping purposes. The \texttt{Helmholtz.py} script is a script in which one can compare the exact solution to the (first and second order) ABC and PML on a homogeneous background. The \texttt{HelmholtzPMLWholeGrid.py} contains a PML implementation that also enlarges the grid to include sources outside the grid. Note that both scripts use coordinate systems different from the C++ implementation. These scripts can be extended to test out, for example, a method for extracting $p_{\text{data}}$ (see next section). 

\subsubsection{Notes and Future Work}
Some notes:

\begin{itemize}
	\item For the best accuracy, one would need at least $5$ and ideally around $10$ grid points per wavelength. For this reason, higher frequencies tend to lead to worse results (given a fixed grid size).
\end{itemize}

There is still work that can be done to improve or expand the finite difference forward model. 

\begin{enumerate}
	\item Second order ABC can be implemented, which are expected to improve the results (see Python script and \cref{sec:ABC}).
	\item Currently the domain is always enlarged so it includes all the source. In a finite difference context this is necessary for computing the pressure field. Perhaps it is possible to find a method to project the effect of the source on the boundary of the subsurface domain, and use that as a boundary condition to solve the pressure field.
	\item Extraction of $p_{\text{data}}$ can be implemented by implementing the receivers via the method given in \cite{Hicks}. This would also require the receivers to always be included in the domain, just as the sources are now always included.
	\item Currently, methods which are required by the Conjugate Gradient inversion model are implemented in the forward model. If an inversion model which does not directly require these methods is implemented, the forward model can be simplified even further (although it then would no longer work with the CG inversion model). After all, the forward model is only there to provide $p_{\text{tot}}$ and $p_{\text{data}}$.
	\item A different method to solve the system of linear equations can be implemented decrease the execution time. Currently, the $LU$-decomposition is used to solve the system. Faster methods could be explored. One possibility would be to use the same $LU$-decomposition, but permute the matrix before factorization to reduce its bandwidth.
\end{enumerate}


\subsection{Solver and Noise}
\label{CG_solver}
\subsubsection{The Conjugate Gradient (CG) Scheme}
\label{conjgrad}
In the inversion scheme, we try to minimise the difference between the
measured data $p_\text{data}$ and the modelled data that is obtained
using the currently best estimate of the contrast function and the
fixed total field.

The residual between the measured and modelled data is obtained as:

\begin{align} \label{eq:residualMeasureModel} r(\xr, \xs, \omega) =
p_{\text{data}}(\xr, \xs, \omega) - \left[\mathcal{K}_\chi
\right](\xr, \xs, \omega),\\
\tag*{\texttt{residualMeasureModel}}
\end{align}

with

\begin{align} \label{eq:modelledDataGreensConv} \left[\mathcal{K}_\chi \right](\xr,
\xs, \omega) = \int \mathcal{G}(\xr, \x, \omega) p_\text{tot}(\x, \xs,
\omega) \chi(\x) \df{\x}\\
\tag*{\texttt{modelledDataGreensConv}}
\end{align}

a linear operator in $\chi$, and where we adopt the specific notation
by Haffinger (Haffinger, Ph.D. thesis 2013, TU Delft) for consistency.
The error functional is equal to

\begin{align} \label{eq:errorFunc} F(\chi) = \eta \int |r(\xr, \xs,
\omega)|^2 \df{\mathbf{x}_\text{s}} \df{\xr}
\df{\omega},\\
\tag*{\texttt{errorFunc}}
\end{align}

withp	

\begin{align} \label{eq:errorFuncSubEtaInv} \eta^{-1} = \int | p_\text{data}(\xr,
\xs, \omega) |^2 \df{\xs} \df{\xr} \df{\omega}, \\
\tag*{\texttt{errorFuncSubEtaInv}} 
 \end{align}

so that for $\chi = 0$ we have $F = 1$. Notice the implicit dependency
on $\chi$.

We want to find a sequence of contrast functions,
$\chi^{(n)}(\vec{x}), n = 1,2,...,$ in which error functional
decreases with increasing iterations. Therefore, after each iteration
the contrast function is updated as:

\begin{align} \label{eq:contrastUpdate} \chi^{(n)}(\vec{x}) =
\chi^{(n-1)}(\vec{x}) + \alpha_n\zeta_n(\vec{x})\\
\tag*{\texttt{contrastUpdate}}
\end{align}

Here, the step size of the update is determined by the parameter
$\alpha_n$ while the update directions are conjugate gradient
directions given by

\begin{align} \label{eq:updateDirectionsCG}\zeta_1(\vec{x}) = g_1(\vec{x})
,\zeta_n(\vec{x}) = g_n(\vec{x}) + \gamma_n\zeta_{n-1}(\vec{x})\\
\tag*{\texttt{updateDirectionsCG}}
\end{align}

The functional derivative w.r.t. $\chi$ in direction $\mathbf{d}$ of
the error functional is equal to
%\begin{subequations}
\begin{align}
\label{eq:functionalDerWRTD}
\partder{F(\mathbf{\chi}, \mathbf{d})}{\mathbf{\chi}} & =
\lim_{\epsilon \rightarrow 0} \frac{F(\mathbf{\chi} + \epsilon
\mathbf{d}) - F(\mathbf{\chi})}{\epsilon} \\
\tag*{\texttt{functionalDerWRTD}}
& = -2 \eta \int \real{\left[\mathcal{K}_\mathbf{d} \right](\xr, \xs,
\omega)^{\dagger} r (\xr, \xs, \omega)} \df{\xs} \df{\xr} \df{\omega}
\end{align}
%\end{subequations}

The derivation of this equation is provided in section
\ref{deriveerrorfunctional}.

For the discrete $\mathcal{K}$ operator the integrand will have the form

\begin{align} \label{eq:integrandForDiscreteK} g_n(\vec{x}) = \eta
\real{[\mathcal{K}^\star\mathbf{r}_{n-1}](\vec{x})}\\
\tag*{\texttt{integrandForDiscreteK}}
\end{align}

where the $\star$ denote element wise multiplication per row and Re
denotes that only the real part will be used.
The adjoint operator $[\mathcal{K}^\star\mathbf{r}_{n-1}]$ can be seen
as a backprojection operator that maps the residual between measured
data and modelled data from the surface domain to its associated
location in the scattering domain.

In our conjugate gradient scheme we make use of the
Polak-Ribi$\grave{e}$re direction,

\begin{align} \label{eq:PolakRibiereDirection} \gamma_n = \frac{\int
g_n(\vec{x})[g_n(\vec{x})-g_{n-1}(\vec{x})]d(\vec{x})}{\int
g_{n-1}(\vec{x}) g_{n-1}(\vec{x})d(\vec{x})}\\
\tag*{\texttt{PolakRibiereDirection}}
\end{align}

The optimal step size is found from the minimisation of cost
functional equation, by setting the derivative equal to zero.
Consequently the optimal step size becomes:

\begin{align} \label{eq:optimalStepSizeCG} \alpha_n = \frac {\real {\int \int
\int r^{\star}_{n-1}(\mathbf{x_\text{r}},\mathbf{x_\text{s}},\omega)[\mathcal{K}
\zeta_n](\mathbf{x_\text{r}},\mathbf{x_\text{s}},\omega)d\vec{x_s}d\vec{x_r}d\omega}}{\int
\int \int \mid[\mathcal{K}
\zeta_n](\mathbf{x_\text{r}},\mathbf{x_\text{s}},\omega) \mid^2
d\vec{x_s}d\vec{x_r}d\omega}\\
\tag*{\texttt{optimalStepSizeCG}}
\end{align}

To initialise the conjugate gradient scheme, we assume, $\chi^0
(\vec{x}) = 0$, leading to
$r_0(\mathbf{x_\text{r}},\mathbf{x_\text{s}},\omega) =
p_{data}(\mathbf{x_\text{r}},\mathbf{x_\text{s}},\omega)$.
Therefore, we find the gradient as :

\begin{align} \label{eq:gradientFunc} g_1(\vec{x}) = \eta
\real{[\mathcal{K}^\star p_\text{data}](\vec{x})}\\
\tag*{\texttt{gradientRunc}}
\end{align}

and we get the first update parameter as :

\begin{align} \label{eq:firstStepSize} \alpha_1 = \frac {\real {\int \int
\int p^{\star}_\text{data}(\mathbf{x_\text{r}},\mathbf{x_\text{s}},\omega)[\mathcal{K}
g_1](\mathbf{x_\text{r}},\mathbf{x_\text{s}},\omega)d\vec{x_s}d\vec{x_r}d\omega}}{\int
\int \int \mid[\mathcal{K}
g_1](\mathbf{x_\text{r}},\mathbf{x_\text{s}},\omega) \mid^2
d\vec{x_s}d\vec{x_r}d\omega}\\
\tag*{\texttt{firstStepSize}}
\end{align}


\subsubsection{Multiplicative Regularisation}
\label{multreg}
Since all seismic data contains some form of noise, the inversion
process needs to be stabilised. Therefore, the CG scheme is extended
in a way that it contains a multiplicative regularisation factor.

The error functional $\mathcal{F}^{tot}$ becomes a product of the
original error functional and a newly introduced regularisation factor
$\mathcal{F}^{reg}$:


\begin{align} \label{eq:errorFuncRegul} \mathcal{F}^{tot}_n =
\mathcal{F}^{data}_n \mathcal{F}^{reg}_n.\\
\tag*{\texttt{errorFuncRegul}}
\end{align}

\textcolor{red}{The structure of the algorithm is going to be heaviliy modified.
During every iteration in the \textit{main loop}, indexed by $n$, we will perform the following operations:}
\begin{enumerate}
\item (Line 39) using the most recent approximation of $\chi_{est} $ (for $n=0$ we have $\chi_{est} \equiv 0$), we update $[\mathcal{K}_\chi]$ following eq. \eqref{eq:modelledDataGreensConv}. 
\textcolor{red}{During this operation what is computed is actually eq. \eqref{eq:modelledDataGreensConv} assuming $\chi \equiv 1$. When invoking $\_forwardModel\rightarrow calculateKappa()$ there are no mentions of $\chi$, but uniquely to $\mathcal{G}$ and $p^{tot}$.
}
\item (Line 40) Update the residual $r_n$ following eq. \eqref{eq:residualMeasureModel}.
\textcolor{red}{\item (Line 43) Update the undocumented parameter $\delta^{ampl}_n = \frac{\delta^{ampl}_{start}}{(n  \delta^{ampl}_{slope}) +1}$.}
\item (Line 52) update $\zeta_n$ according to eq. \eqref{eq:updateDirectionsCG}. 
\textcolor{red}{This way of updating the direction (which is actually called $g$ in the documents) is valid only for the first iteration ever of the algorithm, although here it is clearly applied $n$ times.}
\item (Line 53)  $\alpha_n$ is updated according to eq. \eqref{eq:optimalStepSizeCG} (no regularisation so far).
\item (Line 56) We update $\chi_n$ according to eq. \eqref{eq:contrastUpdate}.
\item (Line 59) We update the residual array and its squared norm using the new $\chi_n$.
\textcolor{red}{\item (Lines 70-106) We enter in an undocumented \textit{inner loop} indexed by $it1$, but that we will index with $1\leq j <  ConjugateGradientWithRegularisationParametersInput.$ $ \_nRegularisationIterations$ for conciseness.}
\end{enumerate}
\textcolor{red}{
Since most all of the 'regularisation' part is computed within the \textit{inner loop}, it seems legit asking what is the purpose of the \textit{main loop}, which also contains operations that should be performed only once (see point 4. of above list).
The only part that is actually unique to the \textit{main loop} is the updating of the $\delta^{ampl}_n$ parameter, used in the computation of the Steering Factor in \textbf{calculateSteeringFactor()}.
\newline
Moreover, as in the inner loop the direction $zeta$ is updated along with the stepSize $alpha$, it required modifications to the whole \textit{StepAndDirection} refactoring, such as creating a class that is both \textit{DirectionCalculator} and \textit{StepSizeCalculator}, thus creating an inheritance diamond issue and forcing to modify the factory for this 'unsplittable' algorithm.
If we were however to follow the algorithm in the Thesis, it could be implemented simply as a \textit{StepSizeCalculator}, which would fit extremely well with the new design.
Please note that currently the regularisation algorithm described in Thesis is not present anywhere in the project.
\newline
}

\textcolor{red}{
\paragraph{Inner Loop}
Once we are inside the inner loop indexed by $j$, we actually take over the enumeration of most quantities already introduced in the \textit{main loop} such as $\chi_{est}, \alpha, \zeta$ and more. Also this mixture of indexing, which is mathematically speaking bad practice, seems to point to the fact that the \textit{main loop} should not in fact exist.
\newline
First, we compute the Regularisation Parameters $b^2_j(\vec{x})$\footnote{please note that $b_j$ is actually a function and not a scalar.}, $\delta^2_j$  and $\_regularisationCurrent.gradient$ by invoking the method \textbf{calculateRegularisationParameters() (Line 72)}:
}

The following weighting function is used \textcolor{red}{(via \textbf{calculateWeightingFactor()})}:

\begin{align} \label{eq:errorFuncRegulWeighting} b_j^{2}(\vec{x}) = (\int_{\vec{x}}
d\vec{x})^{-1} (\mid\nabla\chi_{j-1}(\vec{x})\mid^2 + \delta_{j-1}^2)^{-1}.\\
\tag*{\texttt{errorFuncRegulWeighting}}
\end{align}

In the above equation, the $\delta_{j}^2$ is the steering factor and
\textcolor{red}{should} be defined as:

\begin{align} \label{eq:errorFuncRegulSteering} \delta_{j}^2 = (\int_{\vec{x}}
d\vec{x})^{-1} \int_{\vec{x}} \mid\nabla\chi_{j-1}(\vec{x})\mid^2 d\vec{x},\\
\tag*{\texttt{errorFuncRegulSteering}}
\end{align}
\textcolor{red}{while in fact is (questionably) computed in \textbf{calculateSteeringFactor()} as:
\begin{align}  \delta_j^2 = \frac{1}{2} \delta^{ampl}_n \frac{\int_{(x,z) = \vec{x}}\left(((b_j(\vec{x}) \partial_x \chi_{j-1}(\vec{x}))^2  + (b_j(\vec{x}) \partial_z \chi_{j-1}(\vec{x}))^2\right) d \vec{x}}{\int_{\vec{x}}b_j^2(\vec{x}) d \vec{x}}.
\end{align}
Bringing together the $b_j^2(\vec{x})$ in the upper integral, we obtain
\begin{align}
\delta_{j}^2 = \frac{1}{2} \delta^{ampl}_n \left(\int_{\vec{x}}
b_j^2(\vec{x})d\vec{x}\right)^{-1} \int_{\vec{x}} b_j^2(\vec{x}) \mid\nabla\chi_{j-1}(\vec{x})\mid^2 d\vec{x},
\end{align}
which vaguely resembles eq. \eqref{eq:errorFuncRegulSteering}.
}
\newline

\textcolor{red}{
Observe that here the parameter $\delta^{ampl}_n$ keeps the $n$ index.
}
\textcolor{red}{The last variable that is going to be updated by \textbf{calculateRegularisationParameters()} is $\_regularisationCurrent.gradient$, by invoking \textbf{calculateRegularisationGradient()}:
\begin{align} 
\label{eq:calculateRegularisationGradient}
\_regularisationCurrent.gradient = \partial_x (b^2_{j-1} (\vec{x}) \partial_x \chi_{j-1} (\vec{x}) ) + \partial_z (b^2_{j-1} (\vec{x}) \partial_z \chi_{j-1} (\vec{x}) ).
\end{align}
The presence of these second order derivatives is not very clear, but the method calls a $.gradient$ (Lines 220, 224) on quantities to which was already invoked another $.gradient$ (Line 98, end of \textit{inner loop}). 
Moreover, calling a variable $gradient$ without mentioning it is multiplied by $b_{j-1}^2$ is quite misleading.
}
\newline

\textcolor{red}{
We then proceed to update $\zeta_j$ by computing $g_j$ according to eq. \eqref{eq:integrandForDiscreteK} in the method \textbf{calculateUpdateDirectionRegularisation()}, in which also $gradientCurrent = gradient_j$ gets updated as 
\begin{align}
\_gradient_j = &\eta (\_regularisation_{j-1}.errorFunctional)\real{[\mathcal{K}^\star\mathbf{r}_{n-1}]}(\vec{x}) + \\
&+ (||r_{j-1}||^2)( \_regularisation_j.gradient),
\end{align}
with $\_regularisation_{j-1} =\_regularisationPrevious$,  $\_regularisation_{j} =\_regularisationCurrent$ and\footnote{the variables $\_regularisationPrevious$ and $\_regularisationCurrent$ are declared in the first part of the \textit{main loop} and the method \textbf{calculateRegularisationErrorFunctional()} is called only at the end of the \textit{inner loop}, which means that every time we start the \textit{inner loop} the quantity $\_regularisationPrevious.errorFunctional$ is reinitialized as a \textit{double  0.0}.}
\begin{align}
\_regularisation_{j-1}.errorFunctional =  (\int_{\vec{x}} d\vec{x})^{-1} \int_{\vec{x}} \left(\frac{(|\nabla \chi_j\vec{x}|^2 +\delta^2_{j-1})}{(|\nabla \chi_{j-1}\vec{x}|^2 +\delta^2_{j-1})}  d\vec{x}\right) (cellVolume), \\
\tag*{( calculateRegularisationErrorFunctional() )}
\end{align}
where $cellVolume$ represents the volume of a discretisation cell coming from $\_grid.getCellVolume()$.
Observe how the formula for $errorFunctional_{(j)} $ resembles eq. (2.21) from Thesis for the computation of $F^{reg}_n$, although the \textit{.summation()} operation is done on the whole fraction at once rather than for numerator and denominator separately.
}
\newline

\textcolor{red}{
We now compute new direction $\zeta_j$ by invoking the method \textbf{calculateUpdateDirectionRegularisation()}, which computes $g_j(\vec{x})$ by means of
\begin{align}
gradient_j =& \eta  (\_regularisation_{j-1}.errorFunctional) \ \real{[\mathcal{K}^\star\mathbf{r}_{n-1}]} \\
 &+ (residual_{j-1})( \_regularisation_j.gradient),
\end{align}
which is claimed to be representing eq. (2.25) from Thesis. 
The first term on the right hand side of the above equation matches, 
while in the second term we see that $residual_{j-1} = residualPrevious = ||r_{j-1}||^2$ is linearly dependent to $F ^{data}$ in eq. $(2.8)$ from Thesis. 
Regarding the $ \_regularisation_j.gradient$, we can see that in eq. \eqref{eq:calculateRegularisationGradient} this variable more or less contains a $b_{j-1}^2$ term, although inside of a compositve derivative.
\newline
Once we have obtained $gradient_j= gradientCurrent$, we compute $\gamma_j$ using the Polak-Ribiere formula \eqref{eq:PolakRibiereDirection} and we obtain the new $\zeta_j$ following eq. \eqref{eq:updateDirectionsCG}.
}
\newline
\textcolor{red}{
We then invoke \textbf{calculateStepSizeRegularisation()} where the parameters $\mathcal{A}_0$, $\mathcal{A}_1$, $\mathcal{A}_2$, $\mathcal{B}_0$, $\mathcal{B}_1$ and $\mathcal{B}_2$ that represent the coefficients of the Taylor expansion for $\mathcal{F}^{data}_j$ (the $\mathcal{A}$'s) and $\mathcal{F}^{reg}_j$ (the $\mathcal{B}$'s) are computed.
}

The new total cost functional then becomes a product of two second order polynomials:

\begin{align} \label{eq:errorFuncFourthOrder} \mathcal{F}^{tot}_{\textcolor{red}{j}} \textcolor{red}{(\alpha_j)} =
(\mathcal{A}_2\alpha^{2}_{\textcolor{red}{j}} + \mathcal{A}_1\alpha_{\textcolor{red}{j}}
+\mathcal{A}_0)(\mathcal{B}_2\alpha^{2}_{\textcolor{red}{j}} + \mathcal{B}_1\alpha_{\textcolor{red}{j}} +
\mathcal{B}_0)\\
\tag*{\texttt{errorFuncFourthOrder}}
\end{align}

with the constants A\textsubscript{0}, ..., A\textsubscript{2}, B\textsubscript{2} in appendix \ref{totalcostprefactors}.


\textcolor{red}{
Finally, we find the minimum of eq. \eqref{eq:errorFuncFourthOrder} by computing $\partial_\alpha \mathcal{F}^{tot}_{\textcolor{red}{j}}$ =0, which is computed by finding a real root using the procedure described in subsection (3.8.2) of \textit{Abramowitz, M., and Stegun, I. A., 1970} from Thesis' bibliography).
It is not clear how we are sure that we find the 'correct' real root, as this polynomial could very well possess three distinct real roots.
Moreover, the operations have been unnecessarily modified (although obtaining the expected final value), resulting in arbitrary variables' names (very unclear) and sign and constant multiplications that have no reasons to be. 
\newline
The new $chiEstimate$ is finally updated by means of $chiEstimate += alpha * zeta$ (Line 79), followed by an update of $residualCurrent$ (containing the square norm of the $residual$, Line 82) and $\_regularisationCurrent.gradientChi$ (Line 98).
\newline
There is another Bug (Line 99, just noticed), where a function that should update the $\_regularisationCurrent$ and $\_regularisationPrevious$ is actually invoked with two instances of $\_regularisationPrevious$, thus destroying any hope of validity of the algorithm.
}

\appendix
\section{Appendices}
\subsection{Derivation of the functional derivative of the error functional}
\label{deriveerrorfunctional}
\begin{eqnarray*}
\partder{F(\mathbf{\chi}, \mathbf{d})}{\mathbf{\chi}} & = &
\lim_{\epsilon \rightarrow 0} \frac{F(\mathbf{\chi} + \epsilon
\mathbf{d}) - F(\mathbf{\chi})}{\epsilon} \\
& = & \lim_{\epsilon \rightarrow 0} \frac{\eta}{\epsilon} \int
\left(p_{\text{data}} - \left[\mathcal{K}_\chi \right] - \epsilon
\left[\mathcal{K}_\mathbf{d} \right] \right) \left(p_{\text{data}} -
\left[\mathcal{K}_\chi \right] - \epsilon \left[\mathcal{K}_\mathbf{d}
\right] \right)^{\dagger}\\
& \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \,
\, \, \, \, - & \left(p_{\text{data}} - \left[\mathcal{K}_\chi \right]
\right) \left(p_{\text{data}} - \left[\mathcal{K}_\chi \right]
\right)^{\dagger} \df{\xs} \df{\xr} \df{\omega} \\
& = & -\lim_{\epsilon \rightarrow 0} \frac{\eta}{\epsilon} \int
\epsilon \left[ \left[\mathcal{K}_\mathbf{d} \right]
\left(p_{\text{data}} - \left[\mathcal{K}_\chi \right]
\right)^{\dagger} + \left[\mathcal{K}_\mathbf{d} \right]^{\dagger}
\left(p_{\text{data}} - \left[\mathcal{K}_\chi \right] \right) \right]
\\
& \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \,
\, \, \, \, - & \epsilon^2 \left[\mathcal{K}_\mathbf{d} \right]
\left[\mathcal{K}_\mathbf{d} \right]^{\dagger} \df{\xs} \df{\xr}
\df{\omega} \\
& = & -2 \eta \int \real{\left[\mathcal{K}_\mathbf{d} \right]
\left(p_{\text{data}} - \left[\mathcal{K}_\chi \right]
\right)^{\dagger}} \df{\xs} \df{\xr} \df{\omega} \\
& = & -2 \eta \int \real{\left[\mathcal{K}_\mathbf{d} \right](\xr,
\xs, \omega)^{\dagger} r (\xr, \xs, \omega)} \df{\xs} \df{\xr}
\df{\omega}
\end{eqnarray*}

\subsection{Prefactors of the Total Cost Equation}
\label{totalcostprefactors}
\begin{align} \label{eq:totalCostA2} \mathcal{A}_2 = \eta \int \int \int
\mid \mathcal{K} \zeta_n \mid^2 d\vec{x_s}d\vec{x_r}d\omega\\
\tag*{\texttt{totalCostA2}}
\end{align}

\begin{align} \label{eq:totalCostA1} \mathcal{A}_1 = -2 \eta \real{\int
\int \int r^{\star}_{n-1} \mid \mathcal{K} \zeta_n \mid
d\vec{x_s}d\vec{x_r}d\omega} , \hat{E}\\
\tag*{\texttt{totalCostA1}}
\end{align}

\begin{align} \label{eq:totalCostA0} \mathcal{A}_0 = \eta \int \int \int
\mid r_{n-1} \mid^2 d\vec{x_s}d\vec{x_r}d\omega =
\mathcal{F}^{data}_{n-1} , \hat{E}\\
\tag*{\texttt{totalCostA0}}
 \end{align}

\begin{align} \label{eq:totalCostB2} \mathcal{B}_2 = \mid \mid b_n \nabla
\zeta_n \mid \mid^{2}_D , \hat{E}\\
\tag*{\texttt{totalCostB2}}
\end{align}

\begin{align} \label{eq:totalCostB1} \mathcal{B}_1 = 2 < b_n
\nabla\chi^{n-1}, b_n \nabla \zeta_n >_D, \hat{E}\\
\tag*{\texttt{totalCostB1}}
\end{align}

\begin{align} \label{eq:totalCostB0} \mathcal{B}_0 = \mid \mid b_n \nabla
\chi^{n-1} \mid \mid^{2}_D + \delta^{2}_{n-1} \mid \mid b_n \mid
\mid^{2}_D\\
\tag*{\texttt{totalCostB0}}
\end{align}

\section{Absorbing Boundary Conditions}\label{sec:ABC}
\subsection{Second Order ABC}
To improve the absorption rate of the ABC, we will consider ABC which are second order accurate in the angle at which the wave strikes the boundary \cite{AbsorpationRates}. They can be formulated as follows.

\begin{equation} \label{eqn:SecondOrderABC}
\frac{\partial u}{\partial \hat{\n}} - in\omega u - \frac{i}{2\omega n}\frac{\partial^2 u}{\partial \s^2}=0,
\end{equation}

where $\s$ denotes the vector tangential to the boundary (positive or negative direction does not matter). In the corner specials ABC apply \cite{SecondOrderCorner}

\begin{equation}
\frac{\partial u}{\partial \hat{\s}}-\frac{3}{2}i\omega nu=0,
\end{equation}

where $\hat{\s}$ denotes the vector which is the sum of the tangential outward pointing vectors in the $x$ and $z$ directions.

\subsubsection{Discretization}
Similar to the first order discretization, at the boundary we need to find an expression for the ghost points. Consider, for example, the top boundary ($z=0$). Here $\hat{\n}=[0,-1]^\top$ and $\s=[\pm 1,0]$. Now \cref{eqn:SecondOrderABC} can be discretized as follows.

\begin{eqnarray}
&&\frac{\partial u}{\partial \hat{\n}} - in\omega u - \frac{i}{2\omega n}\frac{\partial^2 u}{\partial \s^2}= -\frac{\partial u}{\partial z} - in\omega u - \frac{i}{2\omega n}\frac{\partial^2 u}{\partial x^2}=0\\
&\leadsto& -\frac{u_{i,j+1}-u_{i,j-1}}{2h_z} - i\omega n_{ij}u_{ij}-\frac{i}{2\omega n_{ij}}\frac{u_{i+1,j}-2u_{ij}+u_{i-1,j}}{h_x^2}=0\\
&\implies& u_{i,j-1} = 2h_z i\omega n_{ij}u_{ij}+u_{i,j+1}+\frac{h_z i}{\omega n_{ij}h_x^2}(u_{i+1,j}-2u_{ij}+u_{i-1,j})=0
\end{eqnarray}

This can be implemented in a straightforward manner by substituting the relation above into \cref{eqn:discret}.\\

For corner points, e.g., $(x,z)=(0,0)$ with $\hat{\s}=[-1,-1]^\top$ it is not possible to fully discretize the $\partial u/\partial\hat{\s}$ using the central difference formula. Then there would be two unknowns, in this case $u_{i-1,j}$ and $u_{i,j-1}$, and hence no straightforward expression for either of these unknowns to be substituted in \cref{eqn:discret}. For this reason, a central difference scheme is only used in the $x$ direction and a backward/forward scheme is used in the $z$ direction. This leads to the following discretization at $(0,0)$.

\begin{eqnarray}
&& \frac{\partial u}{\partial \hat{\s}}-\frac{3}{2}i\omega nu= -\frac{\partial u}{\partial x}-\frac{\partial u}{\partial z}-\frac{3}{2}i\omega nu=0\\
&\leadsto& -\frac{u_{i+1,j}-u_{i-1,j}}{2h_x}-\frac{u_{i,j+1}-u_{i,j}}{h_z}-\frac{3}{2}i\omega n_{ij}u_{ij}=0\\
&\implies& u_{i-1,j} = (3h_xi\omega n_{i,j}-2\frac{h_x}{h_z})u_{i,j}+u_{i+1,j}+2\frac{h_x}{h_z}u_{i,j+1}=0
\end{eqnarray}

At the top right corner, with $\hat{\s}=[1,-1]^\top$, the following discretization is used.

\begin{eqnarray}
&& \frac{\partial u}{\partial \hat{\s}}-\frac{3}{2}i\omega nu= \frac{\partial u}{\partial x}-\frac{\partial u}{\partial z}-\frac{3}{2}i\omega nu=0\\
&\leadsto& \frac{u_{i+1,j}-u_{i-1,j}}{2h_x}-\frac{u_{i,j+1}-u_{i,j}}{h_z}-\frac{3}{2}i\omega n_{ij}u_{ij}=0\\
&\implies& u_{i,j+1} = (3h_xi\omega n_{i,j}-2\frac{h_x}{h_z})u_{i,j}+u_{i-1,j}+2\frac{h_x}{h_z}u_{i,j+1}=0
\end{eqnarray}

\end{document}
